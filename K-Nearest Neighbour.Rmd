---
title: "K-Nearest Neighbor Algorithm"
author: "Dhrubasattwata Roy Choudhury"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-nearest Neighbors Algorithm with Example in R

Let’ s say we are given certain number of objects and each object has its own unique attribute associated with it, for example, you have 5 chairs 10 beds and 15 tables and each for each one of them we know the length, breadth and height. Now, if someone give us a new object with new attributes and ask us to predict as to which category does that new object belongs to, that means we are given the dimension and asked to predict if it is a chair, bed or table, then we would have to use KNN algorithm to determine that.

Therefore, attributes means the property of each object, and each object can be considered as a category. Our job is to check how closely are the properties of the new object is related to any one of the already known categories.When we are given the attributes, we try and plot them on graph. The graphical representation of those attributes will help us to calculate Euclidian distance between the new value and the ones that we already know we have. By doing so, we can be sure as to which category does the new object is closest to.

Points to Remember:  
1. The nearest neighbor you want to check will be called defined by value “k”. If k is 5 then you will check 5 closest neighbors in order to determine the category. If majority of neighbor belongs to a certain category from within those five nearest neighbors, then that will be chosen as the category of upcoming object. Shown in the picture below.  
2. Different variables have different scaling units, like weight in kg and height in cm. Then how do we suppose to use them in the Euclidean formula? Well, we normalize each one of the variables using the formula (x-min(x))/(min(x) — max(x)).  
3. The knn algorithm works well with the numeric variables, this is not to say that it cannot work with categorical variables, but it’s just if you have mix of both categorical and numeric variables as the predictors then it demands little bit of different approach. But if all predictors are numeric, then knn is best because we are dealing with the distance and for that we need hard numbers.  
4. When we split our data into training and testing sets, the data should have already be normalized. That mean we first normalize the data and then split it.  
5. The knn algorithm does not works with ordered-factors in R but rather with factors.  
6. The k-mean algorithm is different than K- nearest neighbor algorithm. K-mean is used for clustering and is a unsupervised learning algorithm whereas Knn is supervised leaning algorithm that works on classification problems.  

### Dataset
We are going to use the IRIS dataset
```{r}
df <- data(iris)
head(iris)
```

Generate the normalize function
```{r}
##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.8 * nrow(iris)) 
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
```

Use the normalize function 

```{r} 
##Run nomalization on first 4 coulumns of dataset because they are the predictors
iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], nor))
summary(iris_norm)
```

###S Split into Training and Test Set
Splitting the dataset into train and test set using the standard of 80% as training set and 20% as test set.
```{r} 
##extract training set
iris_train <- iris_norm[ran,] 
##extract testing set
 iris_test <- iris_norm[-ran,]
```

Extracting columns to be used for KNN matching
```{r} 
##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
 iris_target_category <- iris[ran,5]
 ##extract 5th column if test dataset to measure the accuracy
 iris_test_category <- iris[-ran,5]
```


### BUILD THE K-Nearest Neighbor Model
```{r}
##load the package class
library(class)
##run knn function
pr <- knn(iris_train,iris_test,cl=iris_target_category,k=13)
```

### Performance Metrics: Confusion Matrix, Accuracy
```{r}
##create confusion matrix
tab <- table(pr,iris_test_category)
 
##this function divides the correct predictions by total number of predictions that tell us how accurate teh model is.
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
```

The k-nearest neighbor algorithm that gave 93% accurate result. 

The following steps were done:    
1. Normalized the data to convert petal.length, sepal.length, petal.width and sepal.length into a standardized 0-to-1 form so that we can fit them into one box (one graph) and also because our main objective is to predict whether a flower is virginica, Versicolor, or setosa  (that is excluded the column 5 and stored it into another variable called iris_target_category).   
2. Separated the normalized values into training and testing dataset.     
3. Values from training dataset are firstly drawn on a graph and after we run knn function with all the necessary arguments.  
4. Introduce testing dataset’s values into the graph and calculate Euclidean distance with each and every already stored point in graph. Now, although we know which flower it is in testing dataset, we still predict the values and store them in variable called ‘pr’ so that we can compare predicted values with original testing dataset’s values.   
5. This way we understand the accuracy of our model and if we are to get new 50 values in future and we are asked to predict the category of those 50 values, we can do that with this model.







